{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b93716d4",
   "metadata": {},
   "source": [
    "# Snake AI Model Testing & Evaluation\n",
    "\n",
    "This notebook provides tools for:\n",
    "- Loading and testing trained models\n",
    "- Evaluating model performance on different scenarios\n",
    "- Visualizing model decision-making\n",
    "- Comparing different model versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4e31be69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error importing modules: cannot import name 'get_state' from 'utils' (c:\\Users\\jross\\Source\\ai-snake\\notebooks\\../src\\utils.py)\n",
      "Make sure you're running this from the correct directory and modules exist.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Add the src directory to Python path\n",
    "sys.path.append('..\\src')\n",
    "\n",
    "# Import our Snake AI modules\n",
    "try:\n",
    "    from snake_ai import DQN, SnakeAI\n",
    "    from snake_game import SnakeGame\n",
    "    from utils import get_state\n",
    "    print(\"‚úÖ Snake AI modules imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importing modules: {e}\")\n",
    "    print(\"Make sure you're running this from the correct directory and modules exist.\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fe29f8",
   "metadata": {},
   "source": [
    "## 1. Model Loading and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9f92053f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 model files:\n",
      "  1. SnakeAI/snake_model_best.pth\n",
      "  2. SnakeAI/snake_model_best_best.pth\n",
      "  3. SnakeAI/snake_model_best_checkpoint.pth\n",
      "  4. Snake_AI_Project/best_model.pth\n",
      "  5. Snake_AI_Project/checkpoint.pth\n",
      "  6. Snake_AI_Project_0/best_model.pth\n",
      "  7. Snake_AI_Project_100k/best_model.pth\n",
      "  8. Snake_AI_Project_100k/checkpoint.pth\n",
      "  9. Snake_AI_Project_2/best_model.pth\n",
      "  10. Snake_AI_Project_2/checkpoint.pth\n",
      "  11. Snake_AI_Project_3/best_model.pth\n",
      "  12. Snake_AI_Project_3/checkpoint.pth\n",
      "‚ùå Error loading model: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL numpy._core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy._core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy._core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n"
     ]
    }
   ],
   "source": [
    "def find_model_files():\n",
    "    \"\"\"Find available model files in the project directory.\"\"\"\n",
    "    # Look in the user's roaming data directory\n",
    "    data_path = Path.home() / \"AppData\" / \"Roaming\" / \"SnakeAI\"\n",
    "    \n",
    "    if not data_path.exists():\n",
    "        print(f\"Project data directory not found: {data_path}\")\n",
    "        return []\n",
    "    \n",
    "    # Find all .pth model files\n",
    "    model_files = list(data_path.glob(\"**/models/*.pth\"))\n",
    "    \n",
    "    if not model_files:\n",
    "        print(\"No model files found.\")\n",
    "        print(f\"Looking in: {data_path}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Found {len(model_files)} model files:\")\n",
    "    for i, model_file in enumerate(model_files):\n",
    "        print(f\"  {i+1}. {model_file.parent.parent.name}/{model_file.name}\")\n",
    "    \n",
    "    return model_files\n",
    "\n",
    "def load_model(model_path: Path, input_size: int = 11, output_size: int = 3):\n",
    "    \"\"\"Load a trained model from file.\"\"\"\n",
    "    try:\n",
    "        # Create model architecture\n",
    "        model = DQN(input_size, output_size)\n",
    "        \n",
    "        # Load trained weights\n",
    "        model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "        model.eval()\n",
    "        \n",
    "        print(f\"‚úÖ Model loaded successfully from: {model_path.name}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Find and display available models\n",
    "model_files = find_model_files()\n",
    "\n",
    "# Load the most recent model (or specify index)\n",
    "if model_files:\n",
    "    # Load the most recent model\n",
    "    latest_model_path = max(model_files, key=lambda x: x.stat().st_mtime)\n",
    "    model = load_model(latest_model_path)\n",
    "    \n",
    "    if model is not None:\n",
    "        print(f\"\\nModel architecture:\")\n",
    "        print(model)\n",
    "        \n",
    "        # Count parameters\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"\\nModel parameters:\")\n",
    "        print(f\"  Total: {total_params:,}\")\n",
    "        print(f\"  Trainable: {trainable_params:,}\")\n",
    "else:\n",
    "    model = None\n",
    "    print(\"No models available for testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e157ab",
   "metadata": {},
   "source": [
    "## 2. Model Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6a33196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_performance(model, num_games: int = 100, board_size: int = 10):\n",
    "    \"\"\"Test model performance over multiple games.\"\"\"\n",
    "    if model is None:\n",
    "        print(\"No model loaded for testing.\")\n",
    "        return None\n",
    "    \n",
    "    scores = []\n",
    "    steps_list = []\n",
    "    game_results = []\n",
    "    \n",
    "    print(f\"Testing model performance over {num_games} games...\")\n",
    "    \n",
    "    for game_num in range(num_games):\n",
    "        # Create game instance\n",
    "        game = SnakeGame(board_size, board_size)\n",
    "        game.reset()\n",
    "        \n",
    "        steps = 0\n",
    "        max_steps = board_size * board_size * 2  # Prevent infinite loops\n",
    "        \n",
    "        while not game.game_over and steps < max_steps:\n",
    "            # Get current state\n",
    "            state = get_state(game)\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            \n",
    "            # Get model prediction\n",
    "            with torch.no_grad():\n",
    "                q_values = model(state_tensor)\n",
    "                action = q_values.argmax().item()\n",
    "            \n",
    "            # Take action\n",
    "            game.move(action)\n",
    "            steps += 1\n",
    "        \n",
    "        scores.append(game.score)\n",
    "        steps_list.append(steps)\n",
    "        \n",
    "        # Record detailed game result\n",
    "        game_results.append({\n",
    "            'game': game_num + 1,\n",
    "            'score': game.score,\n",
    "            'steps': steps,\n",
    "            'survived': game.score > 0,\n",
    "            'efficiency': game.score / steps if steps > 0 else 0\n",
    "        })\n",
    "        \n",
    "        if (game_num + 1) % 20 == 0:\n",
    "            print(f\"  Completed {game_num + 1}/{num_games} games...\")\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame(game_results)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    avg_score = np.mean(scores)\n",
    "    max_score = np.max(scores)\n",
    "    success_rate = (np.array(scores) > 0).mean() * 100\n",
    "    avg_steps = np.mean(steps_list)\n",
    "    \n",
    "    print(f\"\\n=== Test Results ===\")\n",
    "    print(f\"Average Score: {avg_score:.2f}\")\n",
    "    print(f\"Best Score: {max_score}\")\n",
    "    print(f\"Success Rate: {success_rate:.1f}%\")\n",
    "    print(f\"Average Steps: {avg_steps:.1f}\")\n",
    "    print(f\"Score Std Dev: {np.std(scores):.2f}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Run performance test\n",
    "if model is not None:\n",
    "    test_results = test_model_performance(model, num_games=50)\n",
    "    \n",
    "    if test_results is not None:\n",
    "        print(\"\\nFirst few test results:\")\n",
    "        display(test_results.head(10))\n",
    "else:\n",
    "    test_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c7072f",
   "metadata": {},
   "source": [
    "## 3. Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d3093b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No test results to visualize.\n"
     ]
    }
   ],
   "source": [
    "# Visualize test results\n",
    "if test_results is not None:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Score distribution\n",
    "    axes[0,0].hist(test_results['score'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0,0].axvline(test_results['score'].mean(), color='red', linestyle='--', \n",
    "                     label=f'Mean: {test_results[\"score\"].mean():.1f}')\n",
    "    axes[0,0].set_title('Score Distribution')\n",
    "    axes[0,0].set_xlabel('Score')\n",
    "    axes[0,0].set_ylabel('Frequency')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Steps vs Score scatter\n",
    "    axes[0,1].scatter(test_results['steps'], test_results['score'], alpha=0.6, color='green')\n",
    "    axes[0,1].set_title('Steps vs Score')\n",
    "    axes[0,1].set_xlabel('Steps Taken')\n",
    "    axes[0,1].set_ylabel('Score Achieved')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add correlation\n",
    "    correlation = test_results['steps'].corr(test_results['score'])\n",
    "    axes[0,1].text(0.05, 0.95, f'Correlation: {correlation:.2f}', \n",
    "                  transform=axes[0,1].transAxes, verticalalignment='top',\n",
    "                  bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Efficiency distribution\n",
    "    axes[1,0].hist(test_results['efficiency'], bins=20, alpha=0.7, color='orange', edgecolor='black')\n",
    "    axes[1,0].axvline(test_results['efficiency'].mean(), color='red', linestyle='--',\n",
    "                     label=f'Mean: {test_results[\"efficiency\"].mean():.3f}')\n",
    "    axes[1,0].set_title('Efficiency Distribution (Score/Steps)')\n",
    "    axes[1,0].set_xlabel('Efficiency')\n",
    "    axes[1,0].set_ylabel('Frequency')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Performance over games (to check consistency)\n",
    "    axes[1,1].plot(test_results['game'], test_results['score'], alpha=0.6, color='purple')\n",
    "    axes[1,1].plot(test_results['game'], test_results['score'].rolling(10, min_periods=1).mean(), \n",
    "                  color='darkred', linewidth=2, label='Rolling Mean (10 games)')\n",
    "    axes[1,1].set_title('Performance Consistency')\n",
    "    axes[1,1].set_xlabel('Game Number')\n",
    "    axes[1,1].set_ylabel('Score')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics table\n",
    "    summary_stats = test_results.describe()\n",
    "    print(\"\\nDetailed Statistics:\")\n",
    "    display(summary_stats)\n",
    "else:\n",
    "    print(\"No test results to visualize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a194c8",
   "metadata": {},
   "source": [
    "## 4. Model Decision Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "52cb41f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_decisions(model, num_states: int = 100):\n",
    "    \"\"\"Analyze what types of decisions the model makes in different situations.\"\"\"\n",
    "    if model is None:\n",
    "        print(\"No model loaded for analysis.\")\n",
    "        return None\n",
    "    \n",
    "    # Action names for interpretation\n",
    "    action_names = ['Straight', 'Right', 'Left']\n",
    "    \n",
    "    # Collect decision data\n",
    "    decision_data = []\n",
    "    \n",
    "    print(f\"Analyzing model decisions over {num_states} random game states...\")\n",
    "    \n",
    "    for i in range(num_states):\n",
    "        # Create a random game state\n",
    "        game = SnakeGame(10, 10)\n",
    "        game.reset()\n",
    "        \n",
    "        # Take a few random moves to create variety\n",
    "        for _ in range(np.random.randint(0, 10)):\n",
    "            if not game.game_over:\n",
    "                game.move(np.random.randint(0, 3))\n",
    "        \n",
    "        if not game.game_over:\n",
    "            # Get state and model decision\n",
    "            state = get_state(game)\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                q_values = model(state_tensor)\n",
    "                action_probs = torch.softmax(q_values, dim=1)\n",
    "                action = q_values.argmax().item()\n",
    "            \n",
    "            # Analyze the state\n",
    "            decision_data.append({\n",
    "                'state_id': i,\n",
    "                'chosen_action': action,\n",
    "                'action_name': action_names[action],\n",
    "                'q_straight': q_values[0][0].item(),\n",
    "                'q_right': q_values[0][1].item(),\n",
    "                'q_left': q_values[0][2].item(),\n",
    "                'confidence': action_probs[0][action].item(),\n",
    "                'danger_straight': state[0],  # Danger straight ahead\n",
    "                'danger_right': state[1],     # Danger to the right\n",
    "                'danger_left': state[2],      # Danger to the left\n",
    "                'food_direction': np.argmax(state[7:11]),  # Food direction\n",
    "                'snake_length': game.score + 1\n",
    "            })\n",
    "    \n",
    "    decisions_df = pd.DataFrame(decision_data)\n",
    "    \n",
    "    print(f\"\\n=== Decision Analysis ===\")\n",
    "    print(f\"Action Distribution:\")\n",
    "    action_counts = decisions_df['action_name'].value_counts()\n",
    "    for action, count in action_counts.items():\n",
    "        print(f\"  {action}: {count} ({count/len(decisions_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Analyze decision confidence\n",
    "    print(f\"\\nAverage Decision Confidence: {decisions_df['confidence'].mean():.2f}\")\n",
    "    \n",
    "    return decisions_df\n",
    "\n",
    "# Analyze model decisions\n",
    "if model is not None:\n",
    "    decision_analysis = analyze_model_decisions(model, num_states=200)\n",
    "    \n",
    "    if decision_analysis is not None:\n",
    "        print(\"\\nFirst few decision records:\")\n",
    "        display(decision_analysis.head())\n",
    "else:\n",
    "    decision_analysis = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6460ab",
   "metadata": {},
   "source": [
    "## 5. Decision Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b2bd7def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No decision analysis data to visualize.\n"
     ]
    }
   ],
   "source": [
    "# Visualize decision patterns\n",
    "if decision_analysis is not None:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Action distribution pie chart\n",
    "    action_counts = decision_analysis['action_name'].value_counts()\n",
    "    axes[0,0].pie(action_counts.values, labels=action_counts.index, autopct='%1.1f%%',\n",
    "                 colors=['lightblue', 'lightgreen', 'lightcoral'])\n",
    "    axes[0,0].set_title('Action Distribution')\n",
    "    \n",
    "    # Decision confidence by action\n",
    "    sns.boxplot(data=decision_analysis, x='action_name', y='confidence', ax=axes[0,1])\n",
    "    axes[0,1].set_title('Decision Confidence by Action')\n",
    "    axes[0,1].set_xlabel('Action')\n",
    "    axes[0,1].set_ylabel('Confidence')\n",
    "    \n",
    "    # Q-values distribution\n",
    "    q_values_cols = ['q_straight', 'q_right', 'q_left']\n",
    "    q_values_data = decision_analysis[q_values_cols]\n",
    "    \n",
    "    axes[1,0].hist([q_values_data['q_straight'], q_values_data['q_right'], q_values_data['q_left']], \n",
    "                  bins=20, alpha=0.7, label=['Straight', 'Right', 'Left'], color=['blue', 'green', 'red'])\n",
    "    axes[1,0].set_title('Q-Values Distribution')\n",
    "    axes[1,0].set_xlabel('Q-Value')\n",
    "    axes[1,0].set_ylabel('Frequency')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Danger vs Action analysis\n",
    "    danger_actions = []\n",
    "    for _, row in decision_analysis.iterrows():\n",
    "        if row['danger_straight'] == 1:\n",
    "            if row['chosen_action'] == 0:  # Chose straight despite danger\n",
    "                danger_actions.append('Risky')\n",
    "            else:\n",
    "                danger_actions.append('Safe')\n",
    "        else:\n",
    "            danger_actions.append('No immediate danger')\n",
    "    \n",
    "    danger_df = pd.DataFrame({'decision_type': danger_actions})\n",
    "    danger_counts = danger_df['decision_type'].value_counts()\n",
    "    \n",
    "    axes[1,1].bar(danger_counts.index, danger_counts.values, \n",
    "                 color=['red', 'green', 'blue'])\n",
    "    axes[1,1].set_title('Danger Response Analysis')\n",
    "    axes[1,1].set_xlabel('Decision Type')\n",
    "    axes[1,1].set_ylabel('Count')\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed analysis\n",
    "    risky_decisions = sum(1 for d in danger_actions if d == 'Risky')\n",
    "    total_danger_situations = sum(1 for d in danger_actions if d != 'No immediate danger')\n",
    "    \n",
    "    if total_danger_situations > 0:\n",
    "        safety_rate = (total_danger_situations - risky_decisions) / total_danger_situations * 100\n",
    "        print(f\"\\nüõ°Ô∏è  Safety Analysis:\")\n",
    "        print(f\"   Safe decisions in danger: {safety_rate:.1f}%\")\n",
    "        print(f\"   Risky decisions: {risky_decisions}/{total_danger_situations}\")\n",
    "    \n",
    "    # Correlation analysis\n",
    "    print(f\"\\nüìä Correlations:\")\n",
    "    print(f\"   Confidence vs Snake Length: {decision_analysis['confidence'].corr(decision_analysis['snake_length']):.2f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No decision analysis data to visualize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242c8597",
   "metadata": {},
   "source": [
    "## 6. Model Comparison (if multiple models available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "be18610a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing 12 available models...\n",
      "\n",
      "Testing model: snake_model_best.pth\n",
      "‚ùå Error loading model: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL numpy._core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy._core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy._core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "\n",
      "Testing model: snake_model_best_best.pth\n",
      "‚ùå Error loading model: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL numpy._core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy._core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy._core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "\n",
      "Testing model: snake_model_best_checkpoint.pth\n",
      "‚ùå Error loading model: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL numpy._core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy._core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy._core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n"
     ]
    }
   ],
   "source": [
    "def compare_models(model_files: List[Path], num_games: int = 50):\n",
    "    \"\"\"Compare performance of multiple models.\"\"\"\n",
    "    if len(model_files) < 2:\n",
    "        print(\"Need at least 2 models for comparison.\")\n",
    "        return None\n",
    "    \n",
    "    comparison_results = []\n",
    "    \n",
    "    for model_path in model_files:\n",
    "        print(f\"\\nTesting model: {model_path.name}\")\n",
    "        \n",
    "        # Load model\n",
    "        test_model = load_model(model_path)\n",
    "        if test_model is None:\n",
    "            continue\n",
    "        \n",
    "        # Test performance\n",
    "        results = test_model_performance(test_model, num_games=num_games)\n",
    "        \n",
    "        if results is not None:\n",
    "            # Calculate summary stats\n",
    "            summary = {\n",
    "                'model_name': model_path.name,\n",
    "                'avg_score': results['score'].mean(),\n",
    "                'max_score': results['score'].max(),\n",
    "                'success_rate': (results['score'] > 0).mean() * 100,\n",
    "                'avg_steps': results['steps'].mean(),\n",
    "                'avg_efficiency': results['efficiency'].mean(),\n",
    "                'score_std': results['score'].std()\n",
    "            }\n",
    "            comparison_results.append(summary)\n",
    "    \n",
    "    if comparison_results:\n",
    "        comparison_df = pd.DataFrame(comparison_results)\n",
    "        \n",
    "        print(\"\\n=== Model Comparison ===\")\n",
    "        display(comparison_df.round(2))\n",
    "        \n",
    "        # Find best model for each metric\n",
    "        print(\"\\nüèÜ Best Models by Metric:\")\n",
    "        metrics = ['avg_score', 'max_score', 'success_rate', 'avg_efficiency']\n",
    "        \n",
    "        for metric in metrics:\n",
    "            best_idx = comparison_df[metric].idxmax()\n",
    "            best_model = comparison_df.loc[best_idx, 'model_name']\n",
    "            best_value = comparison_df.loc[best_idx, metric]\n",
    "            print(f\"   {metric}: {best_model} ({best_value:.2f})\")\n",
    "        \n",
    "        return comparison_df\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Compare models if multiple available\n",
    "if len(model_files) >= 2:\n",
    "    print(f\"Comparing {len(model_files)} available models...\")\n",
    "    model_comparison = compare_models(model_files[:3], num_games=30)  # Limit to 3 models for speed\n",
    "else:\n",
    "    print(f\"Only {len(model_files)} model(s) available. Need at least 2 for comparison.\")\n",
    "    model_comparison = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830bec97",
   "metadata": {},
   "source": [
    "## 7. Save Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3c8e25ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ All results saved to: ..\\model_test_results\n",
      "\n",
      "üéØ Model evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# Save test results for later analysis\n",
    "output_dir = Path('../model_test_results')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "if test_results is not None:\n",
    "    # Save individual test results\n",
    "    test_results.to_csv(output_dir / f'model_test_{timestamp}.csv', index=False)\n",
    "    print(f\"‚úÖ Test results saved to: model_test_{timestamp}.csv\")\n",
    "\n",
    "if decision_analysis is not None:\n",
    "    # Save decision analysis\n",
    "    decision_analysis.to_csv(output_dir / f'decision_analysis_{timestamp}.csv', index=False)\n",
    "    print(f\"‚úÖ Decision analysis saved to: decision_analysis_{timestamp}.csv\")\n",
    "\n",
    "if model_comparison is not None:\n",
    "    # Save model comparison\n",
    "    model_comparison.to_csv(output_dir / f'model_comparison_{timestamp}.csv', index=False)\n",
    "    print(f\"‚úÖ Model comparison saved to: model_comparison_{timestamp}.csv\")\n",
    "\n",
    "print(f\"\\nüìÅ All results saved to: {output_dir}\")\n",
    "print(\"\\nüéØ Model evaluation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
