{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0bbb256",
   "metadata": {},
   "source": [
    "# Snake AI Hyperparameter Experimentation\n",
    "\n",
    "This notebook provides tools for:\n",
    "- Interactive hyperparameter exploration\n",
    "- Running small-scale experiments\n",
    "- Visualizing parameter sensitivity\n",
    "- Prototyping new reward functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c17c3319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error importing modules: cannot import name 'get_state' from 'utils' (c:\\Users\\jross\\Source\\ai-snake\\notebooks\\../src\\utils.py)\n",
      "Make sure you're running this from the correct directory.\n",
      "Hyperparameter experimentation notebook ready!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from typing import Dict, List, Tuple\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Add the src directory to Python path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import our Snake AI modules\n",
    "try:\n",
    "    from snake_ai import DQN, SnakeAI\n",
    "    from snake_game import SnakeGame\n",
    "    from utils import get_state\n",
    "    print(\"‚úÖ Snake AI modules imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importing modules: {e}\")\n",
    "    print(\"Make sure you're running this from the correct directory.\")\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Hyperparameter experimentation notebook ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45739fb",
   "metadata": {},
   "source": [
    "## 1. Define Experimental Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c13aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTrainer:\n",
    "    \"\"\"Simplified trainer for quick hyperparameter experiments.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=11, output_size=3, board_size=10):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.board_size = board_size\n",
    "        \n",
    "    def quick_train(self, hyperparams: Dict, episodes: int = 200) -> Dict:\n",
    "        \"\"\"Run a quick training session with given hyperparameters.\"\"\"\n",
    "        \n",
    "        # Create model and optimizer\n",
    "        model = DQN(self.input_size, self.output_size)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=hyperparams.get('learning_rate', 0.001))\n",
    "        \n",
    "        # Training parameters\n",
    "        epsilon = hyperparams.get('epsilon_start', 1.0)\n",
    "        epsilon_end = hyperparams.get('epsilon_end', 0.01)\n",
    "        epsilon_decay = hyperparams.get('epsilon_decay', 0.995)\n",
    "        \n",
    "        # Reward weights\n",
    "        food_reward = hyperparams.get('food_reward', 100)\n",
    "        survival_reward = hyperparams.get('survival_reward', 0.02)\n",
    "        collision_penalty = hyperparams.get('collision_penalty', -10)\n",
    "        \n",
    "        # Training metrics\n",
    "        scores = []\n",
    "        rewards = []\n",
    "        episode_lengths = []\n",
    "        \n",
    "        for episode in range(episodes):\n",
    "            game = SnakeGame(self.board_size, self.board_size)\n",
    "            game.reset()\n",
    "            \n",
    "            total_reward = 0\n",
    "            steps = 0\n",
    "            \n",
    "            while not game.game_over and steps < 200:  # Limit steps\n",
    "                # Get state\n",
    "                state = get_state(game)\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                \n",
    "                # Choose action (epsilon-greedy)\n",
    "                if np.random.random() < epsilon:\n",
    "                    action = np.random.randint(0, 3)\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        q_values = model(state_tensor)\n",
    "                        action = q_values.argmax().item()\n",
    "                \n",
    "                # Take action\n",
    "                old_score = game.score\n",
    "                game.move(action)\n",
    "                new_score = game.score\n",
    "                \n",
    "                # Calculate reward\n",
    "                reward = 0\n",
    "                if new_score > old_score:  # Ate food\n",
    "                    reward += food_reward\n",
    "                elif game.game_over:  # Collision\n",
    "                    reward += collision_penalty\n",
    "                else:  # Survival\n",
    "                    reward += survival_reward\n",
    "                \n",
    "                total_reward += reward\n",
    "                steps += 1\n",
    "                \n",
    "                # Simple training step (without replay buffer for speed)\n",
    "                if not game.game_over:\n",
    "                    next_state = get_state(game)\n",
    "                    next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        next_q_values = model(next_state_tensor)\n",
    "                        target = reward + 0.99 * next_q_values.max()\n",
    "                else:\n",
    "                    target = reward\n",
    "                \n",
    "                # Update model\n",
    "                q_values = model(state_tensor)\n",
    "                current_q = q_values[0][action]\n",
    "                loss = nn.MSELoss()(current_q, torch.tensor(target, dtype=torch.float32))\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Update epsilon\n",
    "            epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "            \n",
    "            # Record metrics\n",
    "            scores.append(game.score)\n",
    "            rewards.append(total_reward)\n",
    "            episode_lengths.append(steps)\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        avg_score = np.mean(scores[-50:])  # Average of last 50 episodes\n",
    "        max_score = np.max(scores)\n",
    "        success_rate = np.mean([s > 0 for s in scores[-50:]])\n",
    "        convergence_speed = next((i for i, s in enumerate(scores) if s >= 5), episodes)\n",
    "        \n",
    "        return {\n",
    "            'avg_score': avg_score,\n",
    "            'max_score': max_score,\n",
    "            'success_rate': success_rate,\n",
    "            'convergence_speed': convergence_speed,\n",
    "            'final_epsilon': epsilon,\n",
    "            'scores': scores,\n",
    "            'rewards': rewards,\n",
    "            'episode_lengths': episode_lengths\n",
    "        }\n",
    "\n",
    "# Create trainer instance\n",
    "trainer = SimpleTrainer()\n",
    "print(\"Experimental framework ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9bc661",
   "metadata": {},
   "source": [
    "## 2. Single Parameter Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800f973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with learning rates\n",
    "def experiment_learning_rates():\n",
    "    \"\"\"Test different learning rates.\"\"\"\n",
    "    learning_rates = [0.0001, 0.001, 0.005, 0.01, 0.05]\n",
    "    results = []\n",
    "    \n",
    "    print(\"Experimenting with learning rates...\")\n",
    "    \n",
    "    for lr in learning_rates:\n",
    "        print(f\"  Testing learning rate: {lr}\")\n",
    "        \n",
    "        hyperparams = {\n",
    "            'learning_rate': lr,\n",
    "            'epsilon_start': 1.0,\n",
    "            'epsilon_end': 0.01,\n",
    "            'epsilon_decay': 0.995,\n",
    "            'food_reward': 100,\n",
    "            'survival_reward': 0.02,\n",
    "            'collision_penalty': -10\n",
    "        }\n",
    "        \n",
    "        result = trainer.quick_train(hyperparams, episodes=150)\n",
    "        result['learning_rate'] = lr\n",
    "        results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run learning rate experiment\n",
    "lr_results = experiment_learning_rates()\n",
    "print(\"\\nLearning Rate Experiment Results:\")\n",
    "display(lr_results[['learning_rate', 'avg_score', 'max_score', 'success_rate', 'convergence_speed']].round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb41e284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learning rate results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Average score vs learning rate\n",
    "axes[0,0].plot(lr_results['learning_rate'], lr_results['avg_score'], 'bo-', linewidth=2, markersize=8)\n",
    "axes[0,0].set_title('Average Score vs Learning Rate')\n",
    "axes[0,0].set_xlabel('Learning Rate')\n",
    "axes[0,0].set_ylabel('Average Score (Last 50 Episodes)')\n",
    "axes[0,0].set_xscale('log')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Success rate vs learning rate\n",
    "axes[0,1].plot(lr_results['learning_rate'], lr_results['success_rate'], 'ro-', linewidth=2, markersize=8)\n",
    "axes[0,1].set_title('Success Rate vs Learning Rate')\n",
    "axes[0,1].set_xlabel('Learning Rate')\n",
    "axes[0,1].set_ylabel('Success Rate')\n",
    "axes[0,1].set_xscale('log')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Convergence speed vs learning rate\n",
    "axes[1,0].plot(lr_results['learning_rate'], lr_results['convergence_speed'], 'go-', linewidth=2, markersize=8)\n",
    "axes[1,0].set_title('Convergence Speed vs Learning Rate')\n",
    "axes[1,0].set_xlabel('Learning Rate')\n",
    "axes[1,0].set_ylabel('Episodes to First Score ‚â• 5')\n",
    "axes[1,0].set_xscale('log')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Best learning rate summary\n",
    "best_lr_idx = lr_results['avg_score'].idxmax()\n",
    "best_lr = lr_results.loc[best_lr_idx, 'learning_rate']\n",
    "best_score = lr_results.loc[best_lr_idx, 'avg_score']\n",
    "\n",
    "axes[1,1].bar(['Best LR'], [best_score], color='skyblue')\n",
    "axes[1,1].set_title(f'Best Learning Rate: {best_lr}')\n",
    "axes[1,1].set_ylabel('Average Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüèÜ Best learning rate: {best_lr} (avg score: {best_score:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6615ff5",
   "metadata": {},
   "source": [
    "## 3. Reward Weight Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aa2723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with reward weights\n",
    "def experiment_reward_weights():\n",
    "    \"\"\"Test different reward weight combinations.\"\"\"\n",
    "    \n",
    "    # Define parameter ranges\n",
    "    food_rewards = [50, 100, 200]\n",
    "    survival_rewards = [0.01, 0.02, 0.05]\n",
    "    collision_penalties = [-5, -10, -20]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"Experimenting with reward weights...\")\n",
    "    total_combinations = len(food_rewards) * len(survival_rewards) * len(collision_penalties)\n",
    "    current = 0\n",
    "    \n",
    "    for food_r in food_rewards:\n",
    "        for survival_r in survival_rewards:\n",
    "            for collision_p in collision_penalties:\n",
    "                current += 1\n",
    "                print(f\"  Testing combination {current}/{total_combinations}: F={food_r}, S={survival_r}, C={collision_p}\")\n",
    "                \n",
    "                hyperparams = {\n",
    "                    'learning_rate': best_lr,  # Use best learning rate from previous experiment\n",
    "                    'epsilon_start': 1.0,\n",
    "                    'epsilon_end': 0.01,\n",
    "                    'epsilon_decay': 0.995,\n",
    "                    'food_reward': food_r,\n",
    "                    'survival_reward': survival_r,\n",
    "                    'collision_penalty': collision_p\n",
    "                }\n",
    "                \n",
    "                result = trainer.quick_train(hyperparams, episodes=150)\n",
    "                result.update({\n",
    "                    'food_reward': food_r,\n",
    "                    'survival_reward': survival_r,\n",
    "                    'collision_penalty': collision_p\n",
    "                })\n",
    "                results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run reward weight experiment\n",
    "reward_results = experiment_reward_weights()\n",
    "print(\"\\nReward Weight Experiment Results:\")\n",
    "display(reward_results[['food_reward', 'survival_reward', 'collision_penalty', \n",
    "                      'avg_score', 'max_score', 'success_rate']].round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d444b58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive 3D visualization of reward weight exploration\n",
    "fig = go.Figure()\n",
    "\n",
    "# Create 3D scatter plot\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=reward_results['food_reward'],\n",
    "    y=reward_results['survival_reward'],\n",
    "    z=reward_results['collision_penalty'],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=reward_results['avg_score'] * 2,  # Size based on performance\n",
    "        color=reward_results['avg_score'],\n",
    "        colorscale='Viridis',\n",
    "        showscale=True,\n",
    "        colorbar=dict(title=\"Average Score\")\n",
    "    ),\n",
    "    text=[f\"Food: {f}<br>Survival: {s}<br>Collision: {c}<br>Avg Score: {score:.2f}\" \n",
    "          for f, s, c, score in zip(reward_results['food_reward'], \n",
    "                                   reward_results['survival_reward'],\n",
    "                                   reward_results['collision_penalty'],\n",
    "                                   reward_results['avg_score'])],\n",
    "    hovertemplate='%{text}<extra></extra>'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Reward Weight Exploration (3D)',\n",
    "    scene=dict(\n",
    "        xaxis_title='Food Reward',\n",
    "        yaxis_title='Survival Reward',\n",
    "        zaxis_title='Collision Penalty'\n",
    "    ),\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Find best configuration\n",
    "best_config_idx = reward_results['avg_score'].idxmax()\n",
    "best_config = reward_results.loc[best_config_idx]\n",
    "\n",
    "print(f\"\\nüèÜ Best reward configuration:\")\n",
    "print(f\"   Food Reward: {best_config['food_reward']}\")\n",
    "print(f\"   Survival Reward: {best_config['survival_reward']}\")\n",
    "print(f\"   Collision Penalty: {best_config['collision_penalty']}\")\n",
    "print(f\"   Average Score: {best_config['avg_score']:.2f}\")\n",
    "print(f\"   Success Rate: {best_config['success_rate']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb707006",
   "metadata": {},
   "source": [
    "## 4. Epsilon Decay Strategy Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6366ee10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with epsilon decay strategies\n",
    "def experiment_epsilon_strategies():\n",
    "    \"\"\"Test different epsilon decay strategies.\"\"\"\n",
    "    \n",
    "    strategies = [\n",
    "        {'name': 'Slow Decay', 'epsilon_start': 1.0, 'epsilon_end': 0.01, 'epsilon_decay': 0.999},\n",
    "        {'name': 'Medium Decay', 'epsilon_start': 1.0, 'epsilon_end': 0.01, 'epsilon_decay': 0.995},\n",
    "        {'name': 'Fast Decay', 'epsilon_start': 1.0, 'epsilon_end': 0.01, 'epsilon_decay': 0.990},\n",
    "        {'name': 'Very Fast Decay', 'epsilon_start': 1.0, 'epsilon_end': 0.01, 'epsilon_decay': 0.980},\n",
    "        {'name': 'Linear Decay', 'epsilon_start': 1.0, 'epsilon_end': 0.01, 'epsilon_decay': 0.995}  # We'll handle this specially\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"Experimenting with epsilon decay strategies...\")\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        print(f\"  Testing strategy: {strategy['name']}\")\n",
    "        \n",
    "        hyperparams = {\n",
    "            'learning_rate': best_lr,\n",
    "            'epsilon_start': strategy['epsilon_start'],\n",
    "            'epsilon_end': strategy['epsilon_end'],\n",
    "            'epsilon_decay': strategy['epsilon_decay'],\n",
    "            'food_reward': best_config['food_reward'],\n",
    "            'survival_reward': best_config['survival_reward'],\n",
    "            'collision_penalty': best_config['collision_penalty']\n",
    "        }\n",
    "        \n",
    "        result = trainer.quick_train(hyperparams, episodes=200)\n",
    "        result['strategy_name'] = strategy['name']\n",
    "        result['epsilon_decay'] = strategy['epsilon_decay']\n",
    "        results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run epsilon strategy experiment\n",
    "epsilon_results = experiment_epsilon_strategies()\n",
    "print(\"\\nEpsilon Strategy Experiment Results:\")\n",
    "display(epsilon_results[['strategy_name', 'epsilon_decay', 'avg_score', \n",
    "                        'max_score', 'success_rate', 'convergence_speed']].round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba1b491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize epsilon strategy results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Performance by strategy\n",
    "strategy_names = epsilon_results['strategy_name']\n",
    "avg_scores = epsilon_results['avg_score']\n",
    "\n",
    "axes[0,0].bar(strategy_names, avg_scores, color='lightblue')\n",
    "axes[0,0].set_title('Average Score by Epsilon Strategy')\n",
    "axes[0,0].set_ylabel('Average Score')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Convergence speed by strategy\n",
    "axes[0,1].bar(strategy_names, epsilon_results['convergence_speed'], color='lightgreen')\n",
    "axes[0,1].set_title('Convergence Speed by Epsilon Strategy')\n",
    "axes[0,1].set_ylabel('Episodes to Score ‚â• 5')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Success rate by strategy\n",
    "axes[1,0].bar(strategy_names, epsilon_results['success_rate'], color='lightcoral')\n",
    "axes[1,0].set_title('Success Rate by Epsilon Strategy')\n",
    "axes[1,0].set_ylabel('Success Rate')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Learning curves for top strategies\n",
    "top_strategies = epsilon_results.nlargest(3, 'avg_score')\n",
    "for _, strategy in top_strategies.iterrows():\n",
    "    scores = strategy['scores']\n",
    "    rolling_avg = pd.Series(scores).rolling(20, min_periods=1).mean()\n",
    "    axes[1,1].plot(rolling_avg, label=strategy['strategy_name'], alpha=0.8, linewidth=2)\n",
    "\n",
    "axes[1,1].set_title('Learning Curves (Top 3 Strategies)')\n",
    "axes[1,1].set_xlabel('Episode')\n",
    "axes[1,1].set_ylabel('Rolling Average Score (20 episodes)')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find best epsilon strategy\n",
    "best_epsilon_idx = epsilon_results['avg_score'].idxmax()\n",
    "best_epsilon_strategy = epsilon_results.loc[best_epsilon_idx]\n",
    "\n",
    "print(f\"\\nüèÜ Best epsilon strategy: {best_epsilon_strategy['strategy_name']}\")\n",
    "print(f\"   Epsilon decay: {best_epsilon_strategy['epsilon_decay']}\")\n",
    "print(f\"   Average score: {best_epsilon_strategy['avg_score']:.2f}\")\n",
    "print(f\"   Convergence speed: {best_epsilon_strategy['convergence_speed']} episodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec8e312",
   "metadata": {},
   "source": [
    "## 5. Custom Reward Function Prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc07af78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRewardTrainer(SimpleTrainer):\n",
    "    \"\"\"Trainer with custom reward functions for experimentation.\"\"\"\n",
    "    \n",
    "    def distance_based_reward(self, game, old_score, action, distance_to_food_before, distance_to_food_after):\n",
    "        \"\"\"Reward function that considers distance to food.\"\"\"\n",
    "        reward = 0\n",
    "        \n",
    "        # Food reward\n",
    "        if game.score > old_score:\n",
    "            reward += 100\n",
    "        \n",
    "        # Distance reward/penalty\n",
    "        if distance_to_food_after < distance_to_food_before:\n",
    "            reward += 1  # Moving closer to food\n",
    "        elif distance_to_food_after > distance_to_food_before:\n",
    "            reward -= 0.5  # Moving away from food\n",
    "        \n",
    "        # Collision penalty\n",
    "        if game.game_over:\n",
    "            reward -= 10\n",
    "        else:\n",
    "            reward += 0.01  # Survival bonus\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def length_based_reward(self, game, old_score, action, snake_length):\n",
    "        \"\"\"Reward function that scales with snake length.\"\"\"\n",
    "        reward = 0\n",
    "        \n",
    "        # Food reward (scales with length)\n",
    "        if game.score > old_score:\n",
    "            reward += 50 + (snake_length * 10)  # More reward as snake gets longer\n",
    "        \n",
    "        # Collision penalty (also scales with length)\n",
    "        if game.game_over:\n",
    "            reward -= (5 + snake_length * 2)  # Bigger penalty for longer snakes\n",
    "        else:\n",
    "            reward += 0.02  # Survival bonus\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def efficiency_reward(self, game, old_score, action, steps_since_food):\n",
    "        \"\"\"Reward function that encourages efficiency.\"\"\"\n",
    "        reward = 0\n",
    "        \n",
    "        # Food reward (bonus for quick finding)\n",
    "        if game.score > old_score:\n",
    "            efficiency_bonus = max(0, 50 - steps_since_food)  # Bonus decreases with time\n",
    "            reward += 100 + efficiency_bonus\n",
    "        \n",
    "        # Penalty for taking too long\n",
    "        if steps_since_food > 30:\n",
    "            reward -= 0.1\n",
    "        \n",
    "        # Collision penalty\n",
    "        if game.game_over:\n",
    "            reward -= 15\n",
    "        else:\n",
    "            reward += 0.01\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def test_custom_reward(self, reward_function_name, episodes=150):\n",
    "        \"\"\"Test a custom reward function.\"\"\"\n",
    "        model = DQN(self.input_size, self.output_size)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        epsilon = 1.0\n",
    "        epsilon_decay = 0.995\n",
    "        epsilon_end = 0.01\n",
    "        \n",
    "        scores = []\n",
    "        rewards = []\n",
    "        \n",
    "        for episode in range(episodes):\n",
    "            game = SnakeGame(self.board_size, self.board_size)\n",
    "            game.reset()\n",
    "            \n",
    "            total_reward = 0\n",
    "            steps = 0\n",
    "            steps_since_food = 0\n",
    "            \n",
    "            while not game.game_over and steps < 200:\n",
    "                # Get state\n",
    "                state = get_state(game)\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                \n",
    "                # Calculate distance to food (for distance-based reward)\n",
    "                snake_head = (game.snake[0][0], game.snake[0][1])\n",
    "                food_pos = (game.food[0], game.food[1])\n",
    "                distance_before = abs(snake_head[0] - food_pos[0]) + abs(snake_head[1] - food_pos[1])\n",
    "                \n",
    "                # Choose action\n",
    "                if np.random.random() < epsilon:\n",
    "                    action = np.random.randint(0, 3)\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        q_values = model(state_tensor)\n",
    "                        action = q_values.argmax().item()\n",
    "                \n",
    "                # Take action\n",
    "                old_score = game.score\n",
    "                snake_length = len(game.snake)\n",
    "                game.move(action)\n",
    "                \n",
    "                # Calculate distance after move\n",
    "                if not game.game_over:\n",
    "                    snake_head_after = (game.snake[0][0], game.snake[0][1])\n",
    "                    distance_after = abs(snake_head_after[0] - food_pos[0]) + abs(snake_head_after[1] - food_pos[1])\n",
    "                else:\n",
    "                    distance_after = distance_before\n",
    "                \n",
    "                # Calculate custom reward\n",
    "                if reward_function_name == 'distance_based':\n",
    "                    reward = self.distance_based_reward(game, old_score, action, distance_before, distance_after)\n",
    "                elif reward_function_name == 'length_based':\n",
    "                    reward = self.length_based_reward(game, old_score, action, snake_length)\n",
    "                elif reward_function_name == 'efficiency':\n",
    "                    reward = self.efficiency_reward(game, old_score, action, steps_since_food)\n",
    "                else:\n",
    "                    # Default reward\n",
    "                    reward = 100 if game.score > old_score else (-10 if game.game_over else 0.02)\n",
    "                \n",
    "                total_reward += reward\n",
    "                steps += 1\n",
    "                \n",
    "                # Update steps since food\n",
    "                if game.score > old_score:\n",
    "                    steps_since_food = 0\n",
    "                else:\n",
    "                    steps_since_food += 1\n",
    "                \n",
    "                # Training step (simplified)\n",
    "                if not game.game_over:\n",
    "                    next_state = get_state(game)\n",
    "                    next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "                    with torch.no_grad():\n",
    "                        next_q_values = model(next_state_tensor)\n",
    "                        target = reward + 0.99 * next_q_values.max()\n",
    "                else:\n",
    "                    target = reward\n",
    "                \n",
    "                q_values = model(state_tensor)\n",
    "                current_q = q_values[0][action]\n",
    "                loss = nn.MSELoss()(current_q, torch.tensor(target, dtype=torch.float32))\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "            scores.append(game.score)\n",
    "            rewards.append(total_reward)\n",
    "        \n",
    "        return {\n",
    "            'reward_function': reward_function_name,\n",
    "            'avg_score': np.mean(scores[-50:]),\n",
    "            'max_score': np.max(scores),\n",
    "            'success_rate': np.mean([s > 0 for s in scores[-50:]]),\n",
    "            'scores': scores,\n",
    "            'rewards': rewards\n",
    "        }\n",
    "\n",
    "# Test custom reward functions\n",
    "custom_trainer = CustomRewardTrainer()\n",
    "reward_functions = ['distance_based', 'length_based', 'efficiency', 'default']\n",
    "\n",
    "custom_results = []\n",
    "print(\"Testing custom reward functions...\")\n",
    "\n",
    "for rf in reward_functions:\n",
    "    print(f\"  Testing {rf} reward function...\")\n",
    "    result = custom_trainer.test_custom_reward(rf, episodes=150)\n",
    "    custom_results.append(result)\n",
    "\n",
    "custom_df = pd.DataFrame(custom_results)\n",
    "print(\"\\nCustom Reward Function Results:\")\n",
    "display(custom_df[['reward_function', 'avg_score', 'max_score', 'success_rate']].round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c83a735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize custom reward function performance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Performance comparison\n",
    "reward_names = custom_df['reward_function']\n",
    "avg_scores = custom_df['avg_score']\n",
    "\n",
    "colors = ['skyblue', 'lightgreen', 'lightcoral', 'gold']\n",
    "axes[0,0].bar(reward_names, avg_scores, color=colors)\n",
    "axes[0,0].set_title('Average Score by Reward Function')\n",
    "axes[0,0].set_ylabel('Average Score')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Success rate comparison\n",
    "axes[0,1].bar(reward_names, custom_df['success_rate'], color=colors)\n",
    "axes[0,1].set_title('Success Rate by Reward Function')\n",
    "axes[0,1].set_ylabel('Success Rate')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Learning curves\n",
    "for i, result in enumerate(custom_results):\n",
    "    scores = result['scores']\n",
    "    rolling_avg = pd.Series(scores).rolling(20, min_periods=1).mean()\n",
    "    axes[1,0].plot(rolling_avg, label=result['reward_function'], \n",
    "                  color=colors[i], alpha=0.8, linewidth=2)\n",
    "\n",
    "axes[1,0].set_title('Learning Curves Comparison')\n",
    "axes[1,0].set_xlabel('Episode')\n",
    "axes[1,0].set_ylabel('Rolling Average Score (20 episodes)')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Max score comparison\n",
    "axes[1,1].bar(reward_names, custom_df['max_score'], color=colors)\n",
    "axes[1,1].set_title('Maximum Score by Reward Function')\n",
    "axes[1,1].set_ylabel('Maximum Score')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find best custom reward function\n",
    "best_custom_idx = custom_df['avg_score'].idxmax()\n",
    "best_custom = custom_df.loc[best_custom_idx]\n",
    "\n",
    "print(f\"\\nüèÜ Best custom reward function: {best_custom['reward_function']}\")\n",
    "print(f\"   Average score: {best_custom['avg_score']:.2f}\")\n",
    "print(f\"   Max score: {best_custom['max_score']:.0f}\")\n",
    "print(f\"   Success rate: {best_custom['success_rate']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c4045e",
   "metadata": {},
   "source": [
    "## 6. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de31e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"        HYPERPARAMETER EXPERIMENTATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüî¨ EXPERIMENTS CONDUCTED:\")\n",
    "print(f\"   ‚Ä¢ Learning Rate Optimization ({len(lr_results)} configurations)\")\n",
    "print(f\"   ‚Ä¢ Reward Weight Exploration ({len(reward_results)} combinations)\")\n",
    "print(f\"   ‚Ä¢ Epsilon Decay Strategies ({len(epsilon_results)} strategies)\")\n",
    "print(f\"   ‚Ä¢ Custom Reward Functions ({len(custom_results)} functions)\")\n",
    "\n",
    "print(f\"\\nüèÜ BEST CONFIGURATIONS FOUND:\")\n",
    "print(f\"   Learning Rate: {best_lr}\")\n",
    "print(f\"   Reward Weights:\")\n",
    "print(f\"     - Food: {best_config['food_reward']}\")\n",
    "print(f\"     - Survival: {best_config['survival_reward']}\")\n",
    "print(f\"     - Collision: {best_config['collision_penalty']}\")\n",
    "print(f\"   Epsilon Strategy: {best_epsilon_strategy['strategy_name']}\")\n",
    "print(f\"   Custom Reward: {best_custom['reward_function']}\")\n",
    "\n",
    "print(f\"\\nüìä PERFORMANCE SUMMARY:\")\n",
    "print(f\"   Best Average Score: {max(lr_results['avg_score'].max(), reward_results['avg_score'].max(), epsilon_results['avg_score'].max(), custom_df['avg_score'].max()):.2f}\")\n",
    "print(f\"   Best Max Score: {max(lr_results['max_score'].max(), reward_results['max_score'].max(), epsilon_results['max_score'].max(), custom_df['max_score'].max()):.0f}\")\n",
    "print(f\"   Best Success Rate: {max(lr_results['success_rate'].max(), reward_results['success_rate'].max(), epsilon_results['success_rate'].max(), custom_df['success_rate'].max()):.2f}\")\n",
    "\n",
    "print(f\"\\nüí° RECOMMENDATIONS:\")\n",
    "print(f\"   1. Use learning rate: {best_lr} for optimal convergence\")\n",
    "print(f\"   2. Apply reward weights: Food={best_config['food_reward']}, Survival={best_config['survival_reward']}, Collision={best_config['collision_penalty']}\")\n",
    "print(f\"   3. Implement {best_epsilon_strategy['strategy_name'].lower()} epsilon decay\")\n",
    "print(f\"   4. Consider {best_custom['reward_function']} reward function for improved performance\")\n",
    "print(f\"   5. Run longer experiments (500+ episodes) with these optimal settings\")\n",
    "\n",
    "print(f\"\\nüîÑ NEXT STEPS:\")\n",
    "print(f\"   ‚Ä¢ Integrate best hyperparameters into main training script\")\n",
    "print(f\"   ‚Ä¢ Test on different board sizes and configurations\")\n",
    "print(f\"   ‚Ä¢ Experiment with network architecture changes\")\n",
    "print(f\"   ‚Ä¢ Implement replay buffer and target networks\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43246c6e",
   "metadata": {},
   "source": [
    "## 7. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb78fdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all experimental results\n",
    "output_dir = Path('../hyperparameter_experiments')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Save individual experiment results\n",
    "lr_results.to_csv(output_dir / f'learning_rate_experiment_{timestamp}.csv', index=False)\n",
    "reward_results.to_csv(output_dir / f'reward_weights_experiment_{timestamp}.csv', index=False)\n",
    "epsilon_results.to_csv(output_dir / f'epsilon_strategies_experiment_{timestamp}.csv', index=False)\n",
    "custom_df.to_csv(output_dir / f'custom_rewards_experiment_{timestamp}.csv', index=False)\n",
    "\n",
    "# Create optimal configuration file\n",
    "optimal_config = {\n",
    "    'learning_rate': float(best_lr),\n",
    "    'reward_weights': {\n",
    "        'food_reward': int(best_config['food_reward']),\n",
    "        'survival_reward': float(best_config['survival_reward']),\n",
    "        'collision_penalty': int(best_config['collision_penalty'])\n",
    "    },\n",
    "    'epsilon_strategy': {\n",
    "        'name': best_epsilon_strategy['strategy_name'],\n",
    "        'epsilon_start': 1.0,\n",
    "        'epsilon_end': 0.01,\n",
    "        'epsilon_decay': float(best_epsilon_strategy['epsilon_decay'])\n",
    "    },\n",
    "    'best_custom_reward': best_custom['reward_function'],\n",
    "    'experiment_date': timestamp,\n",
    "    'performance_summary': {\n",
    "        'best_avg_score': float(max(lr_results['avg_score'].max(), reward_results['avg_score'].max(), \n",
    "                                   epsilon_results['avg_score'].max(), custom_df['avg_score'].max())),\n",
    "        'best_max_score': int(max(lr_results['max_score'].max(), reward_results['max_score'].max(), \n",
    "                                 epsilon_results['max_score'].max(), custom_df['max_score'].max())),\n",
    "        'best_success_rate': float(max(lr_results['success_rate'].max(), reward_results['success_rate'].max(), \n",
    "                                      epsilon_results['success_rate'].max(), custom_df['success_rate'].max()))\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save optimal configuration as JSON\n",
    "with open(output_dir / f'optimal_hyperparameters_{timestamp}.json', 'w') as f:\n",
    "    json.dump(optimal_config, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ All experimental results saved to: {output_dir}\")\n",
    "print(f\"\\nüìÅ Files created:\")\n",
    "print(f\"   ‚Ä¢ learning_rate_experiment_{timestamp}.csv\")\n",
    "print(f\"   ‚Ä¢ reward_weights_experiment_{timestamp}.csv\")\n",
    "print(f\"   ‚Ä¢ epsilon_strategies_experiment_{timestamp}.csv\")\n",
    "print(f\"   ‚Ä¢ custom_rewards_experiment_{timestamp}.csv\")\n",
    "print(f\"   ‚Ä¢ optimal_hyperparameters_{timestamp}.json\")\n",
    "\n",
    "print(f\"\\nüéØ Hyperparameter experimentation complete!\")\n",
    "print(f\"Use the optimal configuration file to improve your main training setup.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
